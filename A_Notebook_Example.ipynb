{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Neural Network models developed by\n",
    "### Ezequiel Piedras, 2022\n",
    "### Lab4 skeleton and helper functions provided in Reinforcement Learning course were used\n",
    "\n",
    "import gymenv_v2\n",
    "from gymenv_v2 import make_multiple_env\n",
    "from gymenv_v2 import timelimit_wrapper, GurobiOriginalEnv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import wandb\n",
    "\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "\n",
    "    def __init__(self, state_dim, lr, hidden_size):\n",
    "\n",
    "        # Using Keras Functional API to build my custom model\n",
    "        input1 = tf.keras.Input(shape=(state_dim,1))\n",
    "        input2 = tf.keras.Input(shape=(state_dim,1))\n",
    "\n",
    "        lstm1 = tf.keras.layers.LSTM(8)(input1)\n",
    "        lstm2 = tf.keras.layers.LSTM(8)(input2)\n",
    "        dense1a = tf.keras.layers.Dense(hidden_size, activation = \"tanh\")(lstm1)\n",
    "        dense2a = tf.keras.layers.Dense(hidden_size)(dense1a)\n",
    "        dense1b = tf.keras.layers.Dense(hidden_size, activation = \"tanh\")(lstm2)\n",
    "        dense2b = tf.keras.layers.Dense(hidden_size)(dense1b)\n",
    "\n",
    "        output = tf.reduce_mean(tf.tensordot(dense2a,tf.transpose(dense2b), axes = 1), axis = 0)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs = [input1,input2], outputs = output)\n",
    "\n",
    "        # DEFINE THE OPTIMIZER\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "\n",
    "        # RECORD HYPER-PARAMS\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "    def call(self, state):\n",
    "\n",
    "        # Unwrap state tuple\n",
    "        A = state[0]\n",
    "        b = state[1]\n",
    "        # Normalize constraints\n",
    "        A = np.divide(A.transpose(), b).transpose()\n",
    "        A = np.divide(A,(np.max(A,axis=0)-np.min(A,axis=0))) - np.min(A,axis=0)\n",
    "        A = tf.expand_dims(A, axis = 2)\n",
    "        A = tf.cast(A, tf.double)\n",
    "\n",
    "        D = state[3]\n",
    "        e = state[4]\n",
    "        # Normalize cuts\n",
    "        D = np.divide(D.transpose(), e).transpose()\n",
    "        D = np.divide(D, (np.max(D, axis=0) - np.min(D, axis=0))) - np.min(D, axis=0)\n",
    "        D = tf.expand_dims(D, axis = 2)\n",
    "        D = tf.cast(D, tf.double)\n",
    "\n",
    "        # Call model\n",
    "        scores = self.model([A,D])\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def compute_prob(self, state):\n",
    "\n",
    "        prob = tf.cast(tf.nn.softmax(self.call(state), axis = -1), tf.double)  # probabilities computed as softmax, sum to 1\n",
    "        return prob.numpy()\n",
    "\n",
    "    def train(self, states, actions, Es):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # COMPUTE probability vector pi(s) for all s in states\n",
    "            total_loss = 0\n",
    "            N = len(states)\n",
    "\n",
    "            for i in range(N):\n",
    "\n",
    "                prob = tf.cast(tf.nn.softmax(self.call(states[i]), axis=-1), tf.double)\n",
    "                action_onehot = tf.cast(tf.one_hot(actions[i], len(states[i][-1])), tf.double)\n",
    "                prob_selected = tf.reduce_sum(prob * action_onehot, axis=-1)\n",
    "\n",
    "                # FOR ROBUSTNESS\n",
    "                prob_selected += 1e-8\n",
    "\n",
    "                total_loss += -1 * Es[i] * tf.math.log(prob_selected)\n",
    "\n",
    "            # BACKWARD PASS\n",
    "            total_loss_2 = total_loss / N\n",
    "            gradients = tape.gradient(total_loss_2, self.model.trainable_variables)\n",
    "\n",
    "            # UPDATE\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        return total_loss_2.numpy()\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def discounted_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        discounted_r[i] = running_sum * gamma + r[i]\n",
    "        running_sum = discounted_r[i]\n",
    "    return list(discounted_r)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Helper function for testing\n",
    "def make_gurobi_env(load_dir, idx, timelimit):\n",
    "\tprint('loading training instances, dir {} idx {}'.format(load_dir, idx))\n",
    "\tA = np.load('{}/A_{}.npy'.format(load_dir, idx))\n",
    "\tb = np.load('{}/b_{}.npy'.format(load_dir, idx))\n",
    "\tc = np.load('{}/c_{}.npy'.format(load_dir, idx))\n",
    "\tenv = timelimit_wrapper(GurobiOriginalEnv(A, b, c, solution=None, reward_type='obj'), timelimit)\n",
    "\treturn env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "## Training data setup\n",
    "# Setup: You may generate your own instances on which you train the cutting agent.\n",
    "custom_config = {\n",
    "    \"load_dir\"        : 'instances/randomip_n60_m60',   # this is the location of the randomly generated instances (you may specify a different directory)\n",
    "    \"idx_list\"        : list(range(20)),                # take the first 20 instances from the directory\n",
    "    \"timelimit\"       : 50,                             # the maximum horizon length is 50\n",
    "    \"reward_type\"     : 'obj'                           # DO NOT CHANGE reward_type\n",
    "}\n",
    "\n",
    "# Easy Setup: Use the following environment settings. We will evaluate your agent with the same easy config below:\n",
    "easy_config = {\n",
    "    \"load_dir\"        : 'instances/train_10_n60_m60',\n",
    "    \"idx_list\"        : list(range(10)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}\n",
    "\n",
    "# Hard Setup: Use the following environment settings. We will evaluate your agent with the same hard config below:\n",
    "hard_config = {\n",
    "    \"load_dir\"        : 'instances/train_100_n60_m60',\n",
    "    \"idx_list\"        : list(range(99)),\n",
    "    \"timelimit\"       : 50,\n",
    "    \"reward_type\"     : 'obj'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Choose argument (hyper-parameter) for Policy function\n",
    "\n",
    "argument = 8\n",
    "\n",
    "### Choose easy or hard setup\n",
    "dif = \"easy\" # easy OR hard\n",
    "# create env\n",
    "if dif == \"easy\":\n",
    "    env = make_multiple_env(**easy_config)\n",
    "else:\n",
    "    env = make_multiple_env(**hard_config)\n",
    "\n",
    "### TRAINING\n",
    "run = wandb.init(project=\"finalproject\", entity=\"ezequiel-piedras\",\n",
    "                 tags=[\"training-{}\".format(dif)],\n",
    "                 name=\"{dif}-{a:.3f}\".format(dif=dif,a=argument))\n",
    "\n",
    "# Initialize parameters\n",
    "actor_lr = 1e-2  # learning rate for actor\n",
    "numtrajs = 10  # num of trajectories from the current policy to collect in each iteration\n",
    "iterations = 50  # total num of iterations\n",
    "gamma = .1  # discount\n",
    "sigma = 10.\n",
    "state_dim = 60\n",
    "\n",
    "# Instantiate actor model\n",
    "actor = Policy(state_dim, actor_lr, argument)\n",
    "\n",
    "# Training reward history\n",
    "r_history = []\n",
    "\n",
    "for ite in range(iterations):\n",
    "\n",
    "    # For recording trajectories played in this iteration\n",
    "    OBSERVED_STATES = []\n",
    "    ACTIONS = []\n",
    "    MC_VALUES = []\n",
    "\n",
    "    # Record avg. iteration reward\n",
    "    ravgit = np.zeros((numtrajs))\n",
    "\n",
    "    for traj in range(numtrajs):\n",
    "\n",
    "        observed_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        s = env.reset()  # samples a random instance every time env.reset() is called\n",
    "        done = False\n",
    "\n",
    "        t = 0\n",
    "        repisode = 0\n",
    "\n",
    "        while not done:\n",
    "            # Run policy\n",
    "            action_space_size = s[-1].size  # size of action space changes every time\n",
    "            prob = actor.compute_prob(s)\n",
    "            prob = prob.flatten()\n",
    "            prob /= np.sum(prob)\n",
    "            action = np.random.choice(action_space_size, p=prob, size=1)\n",
    "            new_s, r, done, _ = env.step(action)\n",
    "\n",
    "            # Save the move\n",
    "            observed_states.append(s)\n",
    "            actions.append(action[0])\n",
    "            rewards.append(r)\n",
    "\n",
    "            # Update trackers\n",
    "            s = new_s\n",
    "            t += 1\n",
    "            repisode += r\n",
    "\n",
    "        # Record trajectory\n",
    "        V_hats = discounted_rewards(np.array(rewards), gamma)\n",
    "        OBSERVED_STATES.extend(observed_states)\n",
    "        ACTIONS.extend(actions)\n",
    "        MC_VALUES.extend(V_hats)\n",
    "\n",
    "        # Log training reward\n",
    "        ravgit[traj] = repisode\n",
    "\n",
    "    # Update policy model\n",
    "    ES = MC_VALUES + np.random.normal(0, 1, len(MC_VALUES)) / sigma\n",
    "    progr = actor.train(OBSERVED_STATES, ACTIONS, ES)\n",
    "    # Print training progress\n",
    "    ravgit = np.mean(ravgit)\n",
    "    r_history.append(ravgit)\n",
    "    wandb.log({\"Training reward ({} config)\".format(dif): ravgit})\n",
    "    print(\"Argu: {argu:2.3f}    It: {ite:2.0f}   Loss: {progr: .3f}   R: {repisode:.3f}\" \\\n",
    "          .format(argu=argument, ite=ite, progr=progr, repisode=ravgit))\n",
    "\n",
    "r_history = pd.DataFrame(r_history)\n",
    "r_history['MAvg'] = r_history.rolling(5, min_periods=5).mean()\n",
    "#r_history.plot(figsize=(10, 5))\n",
    "\n",
    "\n",
    "### Also test the policy function in the random-generated ILPs\n",
    "r_test = np.zeros((10))\n",
    "for env_i in range(10):\n",
    "    env = make_gurobi_env('instances/randomip_n60_m60', env_i, 100)\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    t = 0\n",
    "    rtestit = 0.\n",
    "    while not d:\n",
    "        # Run policy\n",
    "        action_space_size = s[-1].size  # size of action space changes every time\n",
    "        prob = actor.compute_prob(s)\n",
    "        prob = prob.flatten()\n",
    "        prob /= np.sum(prob)\n",
    "        action = np.random.choice(action_space_size, p=prob, size=1)\n",
    "        s, r, d, _ = env.step(action)\n",
    "        #print('step', t, 'reward', r, 'action space size', s[-1].size)\n",
    "        rtestit += r\n",
    "        t += 1\n",
    "    r_test[env_i] = rtestit\n",
    "    wandb.log({\"Test reward ({} config)\".format(dif): rtestit})\n",
    "\n",
    "print(\"Argu: {argu:2.3f}. Average reward on test set: {avg:.3f}\".\\\n",
    "      format(argu=argument,avg=np.mean(r_test)))\n",
    "wandb.finish()\n",
    "\n",
    "r_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}